---
title: "Pokemon Machine Learning"
author: "Aankur Bhatia"
output: html_notebook
---

Load the required packages

```{r}
all_packages=list("dplyr","caret","ggplot2","mlr","DT")

lapply(all_packages,require,character.only=TRUE)
library(mlr)
```

load the Pokemon dataset next

```{r}
pokemon=read.csv("pokemon_alopez247.csv",stringsAsFactors = TRUE)
```

Summarize data using summarize columns command

```{r}
str(pokemon)
summarizeColumns(pokemon)
summary(pokemon)
head(pokemon)
table(pokemon$hasMegaEvolution)
table(pokemon$isLegendary)

```



Remove columns that will not be used in machine learning

1. Number
2. Name 

```{r}
pokemon$Number=NULL
pokemon$Name=NULL
```

Removing variable that have large missing values - Type_2, Egg_Group_2
```{r}
pokemon$Type_2=NULL
pokemon$Egg_Group_2=NULL
```

Keep only records with hasGender =TRUE

```{r}
library(dplyr)
table(pokemon$hasGender)
pokemon=pokemon %>%
  filter(hasGender=="True")
```

looking at the summary once again

```{r}
summarizeColumns(pokemon)
summary(pokemon)
#remove has Gender
pokemon$hasGender=NULL
str(pokemon)
#find levels of hasMegaEvolution
table(pokemon$hasMegaEvolution)
#mostly false
```

We'll do a multiclassification with Egg_Group1 as the response variable (15 levels) using MLR Package


We will remove variables that have high correlation- multicollinearity. We find predictor correlation

```{r}
#find numeric columns
numcols=pokemon %>% 
  select_if(is.numeric)
varcorr=cor(numcols)
library(corrplot)

corrplot(varcorr,order = "hclust")

highcorr=findCorrelation(varcorr,cutoff = 0.65)
#It shows the first variable 'Total' has high correlation with other variables
pokemon$Total=NULL
```

Let us now preprocess data using center, scale transformations (standardize)

```{r}
pokemon=normalizeFeatures(pokemon,target = "Egg_Group_1",method = "standardize")
summary(pokemon)
```

First let's load the MLR package, and perform data splitting

```{r}
library(mlr)

trainsample=sample(nrow(pokemon),0.6*nrow(pokemon),replace = FALSE)
testsample=setdiff(1:nrow(pokemon),trainsample)
```

Now, we create the classification task for the pokemon dataset

```{r}
#remove the empty factor levels from Egg_Group_1
table(pokemon$Egg_Group_1)
pokemon$Egg_Group_1=factor(pokemon$Egg_Group_1)
pokemontraintask=makeClassifTask(data = pokemon[trainsample,],target = "Egg_Group_1")
pokemontraintask

```


Find learners that can perform multiclass classification. Then create a benchmarking list of at-least 4-5 such learners

```{r}
poklearnersall=listLearners(pokemontraintask,warn.missing.packages = FALSE)[c("class","package")]
#we choose rpart, randomForest, Neural Network, ksvm, C50, gbm for creating the benchmark experiment
pokemonbenchlearners=list(makeLearner("classif.rpart",predict.type = "prob",fix.factors.prediction = TRUE),
                          makeLearner("classif.randomForest",predict.type = "prob",fix.factors.prediction = TRUE),
                          makeLearner("classif.nnet",predict.type = "prob",fix.factors.prediction = TRUE),
                          makeLearner("classif.boosting",predict.type = "prob",fix.factors.prediction = TRUE),
                          makeLearner("classif.C50",predict.type = "prob",fix.factors.prediction = TRUE),
                          makeLearner("classif.gbm",predict.type = "prob",fix.factors.prediction = TRUE))
pokemonbenchlearners
```

Now, we create a benchmark strategy for the resampling. We'll choose cross validation with 2 iterations. We will keep stratify =FALSE since each sample may not have all the classes

```{r}
#Resampling Strategy
pokemonresamplingbench=makeResampleDesc(method = "CV",iters=2)
pokemonresamplingbench
```

Next, we define the performance measures to be estimated for this experiment. Let's take a look at all performance measures and then select appropriate ones for this benchmarking experiment

```{r}
listMeasures(pokemontraintask)
pokemonbenchmeasures=list(kappa,multiclass.brier,acc,ber,logloss,timeboth)
pokemonbenchmeasures
```

Finally, run the benchmark experiment

```{r}
pokemonbench=benchmark(learners = pokemonbenchlearners,tasks = pokemontraintask,resamplings = pokemonresamplingbench,measures = pokemonbenchmeasures)
pokemonbench
#just perform resample using boosting  algorithm (which gave the best results)
boost.learner=makeLearner("classif.boosting",predict.type = "prob",fix.factors.prediction = TRUE)
boost.resamp=makeResampleDesc("CV",iters=10)

boost.resampleexpt=resample(learner = boost.learner,task = pokemontraintask,resampling = boost.resamp,measures = pokemonbenchmeasures,extract = function(x) x$learner.model$variable.importance)
boost.resampleexpt$aggr
boost.resampleexpt$extract
boost.resampleexpt$pred
boost.resampleexpt$measures.test

```

Let's do hyper parameter tuning to achieve better performance. Let's use the classif.booting for the classification. Let's first get all the parameters for classif.boosting algorithm

```{r}
getParamSet("classif.boosting")
```

Now, let's set the hyperparameters using the search parameters

```{r}
boost.search=makeParamSet(
  makeIntegerParam("mfinal",lower = 75,upper = 150),
  makeNumericParam("cp",lower=0.01,upper=0.05),
  makeIntegerParam("maxdepth",lower=10,upper=30)
)

```

Now, find the parameter search algorithm

```{r}
boost.algorithm=makeTuneControlRandom(maxit = 20)
```


Finally, perform the tuning 


```{r}

#run the hyperparameter tuning process
boost.hypertune=tuneParams(learner = boost.learner,task = pokemontraintask,resampling = boost.resamp,measures = pokemonbenchmeasures,par.set = boost.search,control = boost.algorithm)
boost.hypertune
```

Let's store the results of analysis

```{r}
save.image("pokemonresults.RData")
```


Let's say that we want to tune two parameters simultaneously- sigma and cost

```{r}
ps=makeParamSet(
  makeNumericParam("C",lower=-5, upper=5,trafo = function(x) 2^x),
  makeNumericParam("sigma",lower = -5,upper=5,trafo = function(x) 2^x)
)

hypertunesvm=tuneParams(learner = svm.learner,task = svm.task,resampling = rf.resamp,measures = svm.measures,par.set = ps,control = ctrl)
#again tune the parameter
hypertunesvm

```
The result has improved with the best parameters as C=3.72 , sigma=0.0475 

Let's visualize the hyperparameters

```{r}
data2=generateHyperParsEffectData(hypertunesvm,trafo = TRUE)
data2$data
plotHyperParsEffect(data2,x="C",y="sigma",z="rmse.test.rmse",plot.type = "heatmap",
                    interpolate = "regr.earth")

plt=plotHyperParsEffect(data2,x="C",y="sigma",z="rmse.test.rmse",plot.type = "heatmap",
                    interpolate = "regr.earth",show.experiments = TRUE)
min_plt=min(plt$data$rmse.test.rmse)
max_plt=max(plt$data$rmse.test.rmse)
mean_plt=mean(c(min_plt,max_plt))

plt+scale_fill_gradient2(breaks=seq(min_plt,max_plt,length.out = 4),low = "green",mid = "yellow",high = "red",midpoint = mean_plt)
                    
```

